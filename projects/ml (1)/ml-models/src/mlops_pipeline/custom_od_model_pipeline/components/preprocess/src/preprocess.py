# Copyright 2022 Google LLC. This software is provided as-is, without warranty
# or representation for any use or purpose. Your use of it is subject to your
# agreement with Google.
# ==============================================================================

"""This module will convert images and xmls into tfrecord format"""

import os
import argparse
import json
import traceback
import pathlib
import xml.etree.ElementTree as ET
from collections import namedtuple
from typing import List, Tuple
import pandas as pd
import tensorflow.compat.v1 as tf
import dataset_util
import label_map_util
from kfp.v2.components.executor import Executor
from kfp.v2.dsl import Dataset, Output
from google.cloud import storage
import gcsfs
from custlogger import ml_logger
from preprocess_utils import delete_blob, file_exist

os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"


def xml_to_csv(path: str, timestamp: str) -> Tuple[bool, pd.DataFrame]:
    """Iterates through all .xml files (generated by labelImg) in a given
    directory and combines
    them in a single Pandas dataframe.
    Parameters:
    ----------
    path : str
        The path containing the .xml files
    Returns
    -------
    Pandas DataFrame
        The produced dataframe
    """
    fs = gcsfs.GCSFileSystem()
    client = storage.Client()
    csv_flag = False
    csv_path = ""
    files_list = []
    xml_list = []
    files = []
    bucket_name = path.split("/")[2]
    folder = "/".join(path.split("/")[3:])
    try:
        bucket = client.get_bucket(bucket_name)
        elements = bucket.list_blobs(prefix=folder)
    except Exception:
        ml_logger(type_log="ERROR", component="Custom OD-Preporcessing",
                  message="Exception in fetching files from bucket",
                  status_code="500", traceback=traceback.format_exc())
        raise
    all_files = [a.name for a in elements]
    for file_name in all_files:
        if file_name.endswith(".csv") and \
            f"standard_preprocessed_images_{timestamp}.csv" in file_name:
            csv_flag = True
            csv_path = f"gs://{bucket_name}/{file_name}"
    if csv_flag:
        df = pd.read_csv(csv_path)
        files_list = list(df["file_name"])
        for fl in all_files:
            if pathlib.Path(fl).stem in files_list and fl.endswith(".xml"):
                files.append(f"gs://{bucket_name}/{fl}")
        delete_blob(csv_path.split("/")[2], "/".join(csv_path.split("/")[3:]))
    else:
        files = [
            "gs://" + bucket_name + "/" + file_name \
            for file_name in all_files if file_name.endswith(".xml")]
    if len(files) == 0:
        message = "Empty directory, given path: {}".format(path)
        ml_logger(type_log="ERROR", component="Custom OD-Preporcessing",
                  message=message, status_code="500",
                  traceback="Edge case")
        return False, pd.DataFrame()
    for xml_file in files:
        file_pointer = fs.open(xml_file, "r")
        tree = ET.parse(file_pointer)
        root = tree.getroot()
        filename = root.find("filename").text
        width = int(root.find("size").find("width").text)
        height = int(root.find("size").find("height").text)
        for member in root.findall("object"):
            bndbox = member.find("bndbox")
            value = (filename,
                     width,
                     height,
                     member.find("name").text,
                     int(bndbox.find("xmin").text),
                     int(bndbox.find("ymin").text),
                     int(bndbox.find("xmax").text),
                     int(bndbox.find("ymax").text),
                     )
            xml_list.append(value)
    column_name = ["filename", "width", "height",
                   "class", "xmin", "ymin", "xmax", "ymax"]
    xml_df = pd.DataFrame(xml_list, columns=column_name)
    return True, xml_df


def class_text_to_int(label_map_dict: dict, row_label: str) -> int:
    """
    parse the index of label from dict
    Parameters:
    ----------
    label_map_dict : dict
        Dictionary contains label information
    row_label: str
        Label name
    Returns
    -------
    int
        index of label in dict
    """
    return label_map_dict[row_label]


def split(data_frame: pd.DataFrame, group: str) -> List:
    """split dataframe
    Parameters:
    ----------
    data_frame : pd.Dataframe()
        contains all details of images
    group: str
        group the df rows by this value
    Returns
    -------
    groups List
    """
    data = namedtuple("data", ["filename", "object"])
    group_by_data = data_frame.groupby(group)
    return [data(filename, group_by_data.get_group(x)) for filename, x in
            zip(group_by_data.groups.keys(), group_by_data.groups)]


def main(
        xml_dir: str, label_path: str,
        output_path: str, timestamp: str) -> Tuple[bool, int]:
    """Create tf records from  dataset
    Parameters:
    ----------
    xml_dir: str
        The path containing the .xml files
    label_path: str
        labelmap path
    output_path: str
        write tf records to this location
    Returns
    -------
    Boolean values
        status of tf record conversion
    count: int
    """
    try:
        count = 0
        stats = file_exist(label_path)
        if not stats:
            message = "Incorrect labelmap path, Given Path : {}".format(
                label_path)
            ml_logger(type_log="ERROR", component="Custom OD-Preporcessing",
                      message=message, status_code="500",
                      traceback="Edge case")
            return False, 0
        label_map = label_map_util.load_labelmap(label_path)
        label_map_dict = label_map_util.get_label_map_dict(label_map)
        writer = tf.python_io.TFRecordWriter(output_path)
        path = os.path.join(xml_dir)
        conversion_status, examples = xml_to_csv(xml_dir, timestamp)
        if not conversion_status:
            ml_logger(type_log="ERROR", component="Custom OD-Preporcessing",
                      message="xml_to_csv conversion failed",
                      status_code="500", traceback="Edge case")
            return False, 0
        grouped = split(examples, "filename")
        for group in grouped:
            tf_example = dataset_util.create_tf_example(
                group, path, label_map_dict)
            if not tf_example:
                ml_logger(type_log="ERROR",
                          component=f"Skipping file{group.filename}",
                          message="xml_to_csv conversion failed",
                          status_code="500", traceback="Edge case")
                continue
            count += 1
            writer.write(tf_example.SerializeToString())
        writer.close()
        message = "Successfully created the TFRecord file: {}".format(
            output_path)
        ml_logger(type_log="INFO", component="Custom OD-Preporcessing",
                  message=message, status_code="200")
        return True, count
    except Exception:
        ml_logger(type_log="ERROR", component="Custom OD-Preporcessing",
                  message="Exception in main",
                  status_code="500", traceback=traceback.format_exc())
        raise


def preprocess(
        train_path: str, validation_path: str, label_path: str,
        data_pipeline_root: str, output_train: Output[Dataset],
        output_validation: Output[Dataset]):
    """Create tf records from  dataset
    Parameters:
    ----------
    train_path: str
        The path containing train data
    validation_path: str
        path contain validation data
    output_train: Output[Dataset]
        train dataset
    output_validation: Output[Dataset]
        validation dataset
    train_test_count: Output[Artifact]
        contain count train and test documents
    """
    try:
        fs = gcsfs.GCSFileSystem()
        ml_logger(type_log="INFO", component="Custom OD-Preporcessing",
                  message="Preprocessing Started", status_code="200")
        # train tf record generation
        timestamp = data_pipeline_root.split("/")[-1]
        train_tf_record_path = os.path.join(data_pipeline_root, "train.record")
        train_tf_record_result = main(
            train_path, label_path, train_tf_record_path, timestamp)
        if not train_tf_record_result[0]:
            ml_logger(
                type_log="ERROR", component="Custom OD-Preporcessing",
                message="Preprocessing component failed",
                status_code="500", traceback="Edge case")
            raise Exception("Preprocessing component failed")
        # validation tf record generation
        validation_tf_record_path = os.path.join(
            data_pipeline_root, "validation.record")
        validation_tf_record_result = main(
            validation_path, label_path, validation_tf_record_path, timestamp)
        if not validation_tf_record_result[0]:
            ml_logger(
                type_log="ERROR", component="Custom OD-Preporcessing",
                message="Preprocessing component failed",
                status_code="500", traceback="Edge case")
            raise Exception("Preprocessing component failed.")
        output_train.uri = train_tf_record_path
        output_validation.uri = validation_tf_record_path
        train_test_count = {
            "train_size": train_tf_record_result[1],
            "test_size": validation_tf_record_result[1]}
        count_json_path = os.path.join(data_pipeline_root, "data_count.json")
        with fs.open(count_json_path, "w") as json_file:
            json.dump(train_test_count, json_file)
        ml_logger(type_log="INFO", component="Custom OD-Preporcessing",
                  message="Preprocessing Completed Successfully",
                  status_code="200")
    except Exception:
        ml_logger(
            type_log="ERROR", component="Custom OD-Preporcessing",
            message="Exception in Preprocessing, Exiting gracefully",
            status_code="500", traceback=traceback.format_exc())
        raise


def executor_main():
    """Starting point of preprocessing component"""
    parser = argparse.ArgumentParser()
    parser.add_argument("--executor_input", type=str)
    parser.add_argument("--function_to_execute", type=str)
    args, _ = parser.parse_known_args()
    executor_input = json.loads(args.executor_input)
    function_to_execute = globals()[args.function_to_execute]
    executor = Executor(executor_input=executor_input,
                        function_to_execute=function_to_execute)
    executor.execute()


if __name__ == "__main__":
    executor_main()
